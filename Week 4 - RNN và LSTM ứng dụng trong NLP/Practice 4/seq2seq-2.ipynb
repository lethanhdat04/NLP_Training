{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"q6Iu9BoERWaD"},"source":["### **Bài 10: Bài tập về nhà: Ứng dụng mô hình sequence2sequence cho bài toán sinh văn bản (text generation)**\n","Tổng quan: Ở bài tập này chúng ta sẽ ôn lại cách xây dựng và sử dụng mô hình seq2seq cho bài toán sinh văn bản."]},{"cell_type":"markdown","metadata":{"id":"YiPDIh18dWVt"},"source":["**1. Chuẩn bị dữ liệu và tiền xử lý**\n","\n","Trong bài tập này chúng ta sẽ xử lý dữ liệu của bài toán tóm tắt văn bản để thực nghiệm cho bài toán sinh văn bản. Trong bài toán tóm tắt văn bản, input của chương trình sẽ là 1 văn bản dài và output sẽ là 1 văn bản ngắn hơn và chứa những thông tin quan trọng của văn bản đầu vào. Ngược lại với bài toán trên, trong bài toán sinh văn bản, chúng ta muốn input đầu vào là 1 vài keyword hoặc 1 đoạn văn ngắn và output ra 1 đoạn văn dài. Vì thế, ta hoàn toàn có thể sử dụng dữ liệu trong bài toán tóm tắt văn bản để huấn luyện cho bài toán sinh văn bản, với input là câu đã được tóm tắt và output là đoạn văn gốc."]},{"cell_type":"code","metadata":{"id":"8vUbu5jijD8T"},"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9mVYgn2fVy5"},"source":["Bài tập 1: Bạn hãy download bộ dữ liệu tóm tắt văn bản CNN trong link dưới đây và tiền xử lý chúng.\n","*   https://huggingface.co/datasets/cnn_dailymail/tree/main/data?fbclid=IwAR2wYgc5NTa0faUrWfPAlUX8yNX_dPztIK58EiS5tTT12NRwYa_dI7QMu_c\n","\n","\n","Yêu cầu : Sau khi tiền xử lý, chúng ta sẽ có 4 file data gồm :\n","*   train.input.txt : Chứa các câu tóm tắt dùng để huấn luyện mô hình, thường chiếm 80% kích thước tổng dữ liệu\n","*   train.output.txt : Chứa các đoạn văn bản gốc ứng với các tóm tắt.\n","*   valid.input.txt : Chứa các câu tóm tắt dùng để đánh giá mô hình, thường chiếm 10% kích thưởng tổng dữ liệu.\n","*   valid.output.txt : Chứa các đoạn văn bản gốc ứng vs các tóm tắt.\n","\n","Lưu ý : Nếu bạn để max_length của dữ liệu quá lớn, thì mô hình của bạn sẽ rất to và có thể gây tràn RAM.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ai4Rg9N8fL7q"},"source":["# YOUR CODE HERE\n","def create_data(source_path, target_path, number_of_examples):\n","  # YOUR CODE HERE\n","  return source_tensors, target_tensors, source_tokenizer, target_tokenizer\n","\n","number_of_examples = -1\n","train_src_tensors, train_tgt_tensors, train_src_tokenizer, train_tgt_tokenizer = create_data(\"train.input.txt\", \"train.output.txt\", number_of_examples)\n","valid_src_tensors, valid_tgt_tensors, _, _ = create_data(\"valid.input.txt\", \"valid.output.txt\", -1)\n","\n","max_length_source, max_length_target = train_src_tensors.shape[1], train_tgt_tensors.shape[1]\n","\n","print(len(train_src_tensors), len(valid_src_tensors))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jntYQNQDhQJy"},"source":["**2. Sử dụng tf.data.Dataset để tạo dữ liệu huấn luyện**\n","\n","Bài tập 2: Hãy xem lại bài tập seq2seq cho bài toán dịch máy để thực hiện cách build data theo batch."]},{"cell_type":"code","metadata":{"id":"sh86ntmfdQoF"},"source":["BUFFER_SIZE = len(train_src_tensors)\n","BATCH_SIZE = 64\n","\n","steps_per_epoch = len(train_src_tensors)//BATCH_SIZE\n","# YOUR CODE HERE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"boAbxfdYh25B"},"source":["**3. Mô hình Seq2Seq với Attention**\n","\n","Bài tập 3: Hãy viết lại các thành phần Encoder, Attention, Decoder theo cách hiểu của bạn. Ngoài BahdanauAttention, LuongAttention cũng rất phổ biến, bạn hãy thử implement LuongAttention."]},{"cell_type":"code","metadata":{"id":"x6EdqDh2iXg4"},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n","    # YOUR CODE HERE\n","\n","  def call(self, x, hidden):\n","    # YOUR CODE HERE\n","\n","  def initialize_hidden_state(self):\n","    # YOUR CODE HERE\n","\n","class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    # YOUR CODE HERE\n","\n","  def call(self, query, values):\n","    # YOUR CODE HERE\n","\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n","    # YOUR CODE HERE\n","\n","  def call(self, x, hidden, enc_output):\n","    # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rWDlRvCiuk5"},"source":["**4. Hàm tối ưu và hàm lỗi**\n","\n","Hàm tối ưu Adam và hàm lỗi Cross Entropy được dùng rất phổ biến trong các mô hình học sâu, trong phần này, chúng ra sẽ dùng lại các hàm đó nhé."]},{"cell_type":"code","metadata":{"id":"PYs4tec9i_k6"},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UOgKpkcjJXo"},"source":["**5. Huấn luyện**\n","\n","Giờ đã có đủ nguyên liệu và mô hình rồi, ta hãy cùng huấn luyện 1 mô hình có thể sinh văn bản."]},{"cell_type":"code","metadata":{"id":"SuZoHVLqjr70"},"source":["@tf.function\n","def train_step(source, target, enc_hidden):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(source, enc_hidden)\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([train_tgt_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","    for t in range(1, target.shape[1]):\n","      # YOUR CODE HERE\n","\n","  batch_loss = (loss / int(target.shape[1]))\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(loss, variables)\n","  optimizer.apply_gradients(zip(gradients, variables))\n","  return batch_loss\n","\n","\n","checkpoint_dir = './model_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","\n","EPOCHS = 1\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  for (batch, (source, target)) in enumerate(train_dataset.take(steps_per_epoch)):\n","    batch_loss = train_step(source, target, enc_hidden)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                   batch,\n","                                                   batch_loss.numpy()))\n","  # saving (checkpoint) the model every 1 epochs\n","  checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xSClgbvGjMOR"},"source":["**6. Sinh văn bản**\n","\n","Hãy sử dụng mô hình vừa được huấn luyện để thực hiện sinh văn bản."]},{"cell_type":"code","metadata":{"id":"czLOA91zmlCm"},"source":["def evaluate(input_sentence):\n","  # YOUR CODE HERE\n","  return result, source_sentence, attention_plot\n","\n","input_sentence = \"hollywood shores up support for ocean 's thirteen\"\n","original_article = \"hollywood is planning a new sequel to adventure flick `` ocean 's eleven , '' with star george clooney set to reprise his role as a charismatic thief in `` ocean 's thirteen , '' the entertainment press said wednesday .\"\n","result, input_sentence, attention_plot = evaluate(input_sentence)\n","print('Input: %s' % (input_sentence))\n","print('Output : {}'.format(result))"],"execution_count":null,"outputs":[]}]}